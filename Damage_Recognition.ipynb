{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_imshow(title, image):\n",
    "    # convert the image frame BGR to RGB color space and display it\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # define the base path to the input dataset and then use it to derive\n",
    "    # the path to the images directory and annotation CSV file\n",
    "    BASE_PATH = \"C:/Users/HP/DF2009CM/capstone_project/dataset\"\n",
    "    IMAGES_PATH = os.path.sep.join([BASE_PATH, \"images\"])\n",
    "    ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"annotations\"])\n",
    "\n",
    "    # define the path to the base output directory\n",
    "    BASE_OUTPUT = \"output\"\n",
    "\n",
    "    # define the path to the output serialized model, model training plot,\n",
    "    # and testing image filenames\n",
    "    MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.h5\"])\n",
    "    LB_PATH = os.path.sep.join([BASE_OUTPUT, \"lb.pickle\"])\n",
    "    PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plots\"])\n",
    "    TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])\n",
    "\n",
    "    # initialize our initial learning rate, number of epochs to train\n",
    "    # for, and the batch size\n",
    "    INIT_LR = 1e-3\n",
    "    NUM_EPOCHS = 20\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "# instantiate the config class\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# load the contents of the CSV annotations file\n",
    "print(\"[INFO] loading dataset...\")\n",
    "\n",
    "# initialize the list of data (images), our target output predictions\n",
    "# (bounding box coordinates), along with the filenames of the\n",
    "# individual images\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "bboxes = []\n",
    "imagePaths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all CSV files in the annotations directory\n",
    "for csvPath in paths.list_files(config.ANNOTS_PATH, validExts=(\".csv\")):\n",
    "    # load the contents of the current CSV annotations file\n",
    "    rows = open(csvPath).read().strip().split(\"\\n\")\n",
    "    # loop over the rows\n",
    "    for row in rows:\n",
    "        # break the row into the filename, bounding box coordinates,\n",
    "        # and class label\n",
    "        row = row.split(\",\")\n",
    "        (filename, startX, startY, endX, endY, label) = row\n",
    "        \n",
    "        # derive the path to the input image, load the image (in\n",
    "        # OpenCV format), and grab its dimensions\n",
    "        imagePath = os.path.sep.join([config.IMAGES_PATH, label,\n",
    "            filename])\n",
    "        image = cv2.imread(imagePath)\n",
    "        (h, w) = image.shape[:2]\n",
    "        # scale the bounding box coordinates relative to the spatial\n",
    "        # dimensions of the input image\n",
    "        startX = float(startX) / w\n",
    "        startY = float(startY) / h\n",
    "        endX = float(endX) / w\n",
    "        endY = float(endY) / h\n",
    "        \n",
    "        # load the image and preprocess it\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.resize(image,(224,224))\n",
    "        image = np.array(image)\n",
    "        # update our list of data, class labels, bounding boxes, and\n",
    "        # image paths\n",
    "        data.append(image.astype('float32'))\n",
    "        labels.append(label)\n",
    "        bboxes.append((startX, startY, endX, endY))\n",
    "        imagePaths.append(imagePath)\n",
    "        \n",
    "data = np.asarray(data)\n",
    "        \n",
    "# convert the data, class labels, bounding boxes, and image paths to\n",
    "# NumPy arrays, scaling the input pixel intensities from the range\n",
    "# [0, 255] to [0, 1]\n",
    "data /= 255.0\n",
    "labels = np.array(labels)\n",
    "bboxes = np.array(bboxes, dtype=\"float32\")\n",
    "imagePaths = np.array(imagePaths)\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "# only there are only two labels in the dataset, then we need to use\n",
    "# Keras/TensorFlow's utility function as well\n",
    "if len(lb.classes_) == 2:\n",
    "    labels = to_categorical(labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving testing image paths...\n"
     ]
    }
   ],
   "source": [
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "split = train_test_split(data, labels, bboxes, imagePaths,\n",
    "    test_size=0.20, random_state=42)\n",
    "# unpack the data split\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainLabels, testLabels) = split[2:4]\n",
    "(trainBBoxes, testBBoxes) = split[4:6]\n",
    "(trainPaths, testPaths) = split[6:]\n",
    "# write the testing image paths to disk so that we can use then\n",
    "# when evaluating/testing our object detector\n",
    "print(\"[INFO] saving testing image paths...\")\n",
    "f = open(config.TEST_PATHS, \"w\")\n",
    "f.write(\"\\n\".join(testPaths))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the VGG16 network, ensuring the head FC layers are left off\n",
    "vgg = VGG16(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "# freeze all VGG layers so they will *not* be updated during the\n",
    "# training process\n",
    "vgg.trainable = False\n",
    "# flatten the max-pooling output of VGG\n",
    "flatten = vgg.output\n",
    "flatten = Flatten()(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 25088)        0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          12845568    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          3211392     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bounding_box (Dense)            (None, 4)            132         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "class_label (Dense)             (None, 4)            2052        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,046,824\n",
      "Trainable params: 16,332,136\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# construct a fully-connected layer header to output the predicted\n",
    "# bounding box coordinates\n",
    "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
    "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(4, activation=\"sigmoid\",\n",
    "    name=\"bounding_box\")(bboxHead)\n",
    "# construct a second fully-connected layer head, this one to predict\n",
    "# the class label\n",
    "softmaxHead = Dense(512, activation=\"relu\")(flatten)\n",
    "softmaxHead = Dropout(0.3)(softmaxHead)\n",
    "softmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\n",
    "softmaxHead = Dropout(0.3)(softmaxHead)\n",
    "softmaxHead = Dense(len(lb.classes_), activation=\"softmax\",\n",
    "    name=\"class_label\")(softmaxHead)\n",
    "\n",
    "# put together our model which accept an input image and then output\n",
    "# bounding box coordinates and a class label\n",
    "model = Model(\n",
    "    inputs=vgg.input,\n",
    "    outputs=(bboxHead, softmaxHead))\n",
    "# define a dictionary to set the loss methods -- categorical\n",
    "# cross-entropy for the class label head and mean absolute error\n",
    "# for the bounding box head\n",
    "losses = {\n",
    "    \"class_label\": \"categorical_crossentropy\",\n",
    "    \"bounding_box\": \"mean_squared_error\",\n",
    "}\n",
    "# define a dictionary that specifies the weights per loss (both the\n",
    "# class label and bounding box outputs will receive equal weight)\n",
    "lossWeights = {\n",
    "    \"class_label\": 1.0,\n",
    "    \"bounding_box\": 1.0\n",
    "}\n",
    "# initialize the optimizer, compile the model, and show the model\n",
    "# summary\n",
    "opt = Adam(learning_rate=config.INIT_LR)\n",
    "model.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=lossWeights)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/20\n",
      "14/14 [==============================] - 48s 3s/step - loss: 3.5617 - bounding_box_loss: 0.0602 - class_label_loss: 3.5015 - bounding_box_accuracy: 0.4378 - class_label_accuracy: 0.3134 - val_loss: 0.9375 - val_bounding_box_loss: 0.0431 - val_class_label_loss: 0.8944 - val_bounding_box_accuracy: 0.5780 - val_class_label_accuracy: 0.6055\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 1.1543 - bounding_box_loss: 0.0354 - class_label_loss: 1.1189 - bounding_box_accuracy: 0.5737 - class_label_accuracy: 0.5622 - val_loss: 0.8453 - val_bounding_box_loss: 0.0362 - val_class_label_loss: 0.8091 - val_bounding_box_accuracy: 0.6881 - val_class_label_accuracy: 0.6422\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 48s 3s/step - loss: 0.6744 - bounding_box_loss: 0.0211 - class_label_loss: 0.6533 - bounding_box_accuracy: 0.6267 - class_label_accuracy: 0.7212 - val_loss: 0.7828 - val_bounding_box_loss: 0.0285 - val_class_label_loss: 0.7543 - val_bounding_box_accuracy: 0.6789 - val_class_label_accuracy: 0.6697\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.4423 - bounding_box_loss: 0.0153 - class_label_loss: 0.4270 - bounding_box_accuracy: 0.7051 - class_label_accuracy: 0.8410 - val_loss: 0.6925 - val_bounding_box_loss: 0.0297 - val_class_label_loss: 0.6628 - val_bounding_box_accuracy: 0.5596 - val_class_label_accuracy: 0.7156\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 51s 4s/step - loss: 0.2582 - bounding_box_loss: 0.0105 - class_label_loss: 0.2476 - bounding_box_accuracy: 0.7581 - class_label_accuracy: 0.9171 - val_loss: 0.5883 - val_bounding_box_loss: 0.0272 - val_class_label_loss: 0.5611 - val_bounding_box_accuracy: 0.7431 - val_class_label_accuracy: 0.7890\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.1734 - bounding_box_loss: 0.0080 - class_label_loss: 0.1654 - bounding_box_accuracy: 0.8157 - class_label_accuracy: 0.9516 - val_loss: 0.6282 - val_bounding_box_loss: 0.0264 - val_class_label_loss: 0.6017 - val_bounding_box_accuracy: 0.7156 - val_class_label_accuracy: 0.7982\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.1406 - bounding_box_loss: 0.0059 - class_label_loss: 0.1347 - bounding_box_accuracy: 0.8479 - class_label_accuracy: 0.9654 - val_loss: 0.6580 - val_bounding_box_loss: 0.0261 - val_class_label_loss: 0.6319 - val_bounding_box_accuracy: 0.7798 - val_class_label_accuracy: 0.7431\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.0891 - bounding_box_loss: 0.0054 - class_label_loss: 0.0837 - bounding_box_accuracy: 0.8802 - class_label_accuracy: 0.9724 - val_loss: 0.7145 - val_bounding_box_loss: 0.0273 - val_class_label_loss: 0.6873 - val_bounding_box_accuracy: 0.7248 - val_class_label_accuracy: 0.7890\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 46s 3s/step - loss: 0.0752 - bounding_box_loss: 0.0055 - class_label_loss: 0.0698 - bounding_box_accuracy: 0.8802 - class_label_accuracy: 0.9770 - val_loss: 0.9051 - val_bounding_box_loss: 0.0267 - val_class_label_loss: 0.8784 - val_bounding_box_accuracy: 0.7339 - val_class_label_accuracy: 0.7431\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 46s 3s/step - loss: 0.0885 - bounding_box_loss: 0.0048 - class_label_loss: 0.0837 - bounding_box_accuracy: 0.8618 - class_label_accuracy: 0.9816 - val_loss: 0.7314 - val_bounding_box_loss: 0.0258 - val_class_label_loss: 0.7055 - val_bounding_box_accuracy: 0.7523 - val_class_label_accuracy: 0.7890\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.0491 - bounding_box_loss: 0.0047 - class_label_loss: 0.0443 - bounding_box_accuracy: 0.8940 - class_label_accuracy: 0.9839 - val_loss: 0.9177 - val_bounding_box_loss: 0.0270 - val_class_label_loss: 0.8907 - val_bounding_box_accuracy: 0.7248 - val_class_label_accuracy: 0.7798\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.0597 - bounding_box_loss: 0.0040 - class_label_loss: 0.0558 - bounding_box_accuracy: 0.8940 - class_label_accuracy: 0.9862 - val_loss: 0.8557 - val_bounding_box_loss: 0.0259 - val_class_label_loss: 0.8299 - val_bounding_box_accuracy: 0.7339 - val_class_label_accuracy: 0.7339\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 48s 3s/step - loss: 0.0392 - bounding_box_loss: 0.0037 - class_label_loss: 0.0355 - bounding_box_accuracy: 0.8756 - class_label_accuracy: 0.9954 - val_loss: 0.8756 - val_bounding_box_loss: 0.0269 - val_class_label_loss: 0.8487 - val_bounding_box_accuracy: 0.7156 - val_class_label_accuracy: 0.7982\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 48s 3s/step - loss: 0.0212 - bounding_box_loss: 0.0032 - class_label_loss: 0.0180 - bounding_box_accuracy: 0.8986 - class_label_accuracy: 0.9954 - val_loss: 0.7258 - val_bounding_box_loss: 0.0271 - val_class_label_loss: 0.6987 - val_bounding_box_accuracy: 0.7523 - val_class_label_accuracy: 0.8349\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.0150 - bounding_box_loss: 0.0033 - class_label_loss: 0.0118 - bounding_box_accuracy: 0.8871 - class_label_accuracy: 0.9954 - val_loss: 0.8023 - val_bounding_box_loss: 0.0270 - val_class_label_loss: 0.7753 - val_bounding_box_accuracy: 0.7523 - val_class_label_accuracy: 0.8440\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 48s 3s/step - loss: 0.0189 - bounding_box_loss: 0.0027 - class_label_loss: 0.0162 - bounding_box_accuracy: 0.8871 - class_label_accuracy: 0.9931 - val_loss: 0.8328 - val_bounding_box_loss: 0.0269 - val_class_label_loss: 0.8059 - val_bounding_box_accuracy: 0.7523 - val_class_label_accuracy: 0.8165\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.0161 - bounding_box_loss: 0.0027 - class_label_loss: 0.0133 - bounding_box_accuracy: 0.9032 - class_label_accuracy: 0.9977 - val_loss: 0.7998 - val_bounding_box_loss: 0.0264 - val_class_label_loss: 0.7734 - val_bounding_box_accuracy: 0.7523 - val_class_label_accuracy: 0.8073\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 47s 3s/step - loss: 0.0225 - bounding_box_loss: 0.0024 - class_label_loss: 0.0201 - bounding_box_accuracy: 0.9101 - class_label_accuracy: 0.9931 - val_loss: 0.9385 - val_bounding_box_loss: 0.0264 - val_class_label_loss: 0.9121 - val_bounding_box_accuracy: 0.7523 - val_class_label_accuracy: 0.7982\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 49s 4s/step - loss: 0.0251 - bounding_box_loss: 0.0023 - class_label_loss: 0.0228 - bounding_box_accuracy: 0.9263 - class_label_accuracy: 0.9954 - val_loss: 0.8250 - val_bounding_box_loss: 0.0263 - val_class_label_loss: 0.7987 - val_bounding_box_accuracy: 0.7248 - val_class_label_accuracy: 0.8257\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 48s 3s/step - loss: 0.0240 - bounding_box_loss: 0.0026 - class_label_loss: 0.0214 - bounding_box_accuracy: 0.9032 - class_label_accuracy: 0.9908 - val_loss: 0.8495 - val_bounding_box_loss: 0.0273 - val_class_label_loss: 0.8222 - val_bounding_box_accuracy: 0.7523 - val_class_label_accuracy: 0.7798\n",
      "[INFO] saving object detector model...\n",
      "[INFO] saving label binarizer...\n"
     ]
    }
   ],
   "source": [
    "# construct a dictionary for our target training outputs\n",
    "trainTargets = {\n",
    "    \"class_label\": trainLabels,\n",
    "    \"bounding_box\": trainBBoxes\n",
    "}\n",
    "# construct a second dictionary, this one for our target testing\n",
    "# outputs\n",
    "testTargets = {\n",
    "    \"class_label\": testLabels,\n",
    "    \"bounding_box\": testBBoxes\n",
    "}\n",
    "\n",
    "# train the network for bounding box regression and class label\n",
    "# prediction\n",
    "print(\"[INFO] training model...\")\n",
    "H = model.fit(\n",
    "    trainImages, trainTargets,\n",
    "    validation_data=(testImages, testTargets),\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    epochs=config.NUM_EPOCHS,\n",
    "    verbose=1)\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "model.save(config.MODEL_PATH, save_format=\"h5\")\n",
    "# serialize the label binarizer to disk\n",
    "print(\"[INFO] saving label binarizer...\")\n",
    "f = open(config.LB_PATH, \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the total loss, label loss, and bounding box loss\n",
    "lossNames = [\"loss\", \"class_label_loss\", \"bounding_box_loss\"]\n",
    "N = np.arange(0, config.NUM_EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
    "# loop over the loss names\n",
    "for (i, l) in enumerate(lossNames):\n",
    "    # plot the loss for both the training and validation data\n",
    "    title = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
    "    ax[i].set_title(title)\n",
    "    ax[i].set_xlabel(\"Epoch #\")\n",
    "    ax[i].set_ylabel(\"Loss\")\n",
    "    ax[i].plot(N, H.history[l], label=l)\n",
    "    ax[i].plot(N, H.history[\"val_\" + l], label=\"val_\" + l)\n",
    "    ax[i].legend()\n",
    "# save the losses figure and create a new figure for the accuracies\n",
    "plt.tight_layout()\n",
    "plotPath = os.path.sep.join([config.PLOT_PATH, \"losses.png\"])\n",
    "plt.savefig(plotPath)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to create images\n",
    "\n",
    "def create_images(name,save_path,image_path):\n",
    "    #Generating a random number for rotation\n",
    "    rotation = random.randint(-20,20)\n",
    "    #Creating Image Data Generator:\n",
    "    augmentation = ImageDataGenerator(rotation_range=rotation,horizontal_flip=True)\n",
    "    \n",
    "    #Reading the image from given path.\n",
    "    image_org = cv2.imread(image_path+'/'+name)\n",
    "    \n",
    "    #Saving original image to folder.\n",
    "    #image_org.save(save_path+'/'+name)\n",
    "    \n",
    "    #Getting the numpy array of image.\n",
    "    image_arr = np.array(image_org)\n",
    "    #image_arr = image.img_to_array(image_org)\n",
    "    \n",
    "    #Expanding dimensions of image array\n",
    "    image_arr = image_arr.reshape((1,) + image_arr.shape)\n",
    "    \n",
    "    #Using Flow to generate 2 new images for single image\n",
    "    for arr,val in zip(augmentation.flow(image_arr, batch_size=1, save_format='jpeg'),range(1)):\n",
    "        name = name.split('.')[0]\n",
    "        img_save = image.array_to_img(arr[0], scale=False)\n",
    "        img_save.save(save_path+'/z_'+name+'_aug_'+str(val)+'.jpeg')\n",
    "    #return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratches_list = os.listdir('C:/Users/HP/DF2009CM/capstone_project/dataset/images/Scratches')\n",
    "scratches_aug = \"dataset/aug_images/Scratches\"\n",
    "\n",
    "for i in tqdm(scratches_list):\n",
    "    create_images(i,scratches_aug,'C:/Users/HP/DF2009CM/capstone_project/dataset/images/Scratches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratches_list = os.listdir('C:/Users/HP/DF2009CM/capstone_project/dataset/images/Dents')\n",
    "scratches_aug = \"dataset/aug_images/Dents\"\n",
    "\n",
    "for i in tqdm(scratches_list):\n",
    "    create_images(i,scratches_aug,'C:/Users/HP/DF2009CM/capstone_project/dataset/images/Dents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
